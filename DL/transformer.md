# Transformer

## 概述
用于 sequence to sequence的问题
> 例如输入一段英语，翻译成中文

工作:
- 第一个基于注意力机制的序列转录模型（将原来的循环层，全部更换成multi-headed self-attention(多头注意力机制)

结论：
- 比原来的学习模型效果要好
- 注意力机制可以更多用于模型

## 导论

